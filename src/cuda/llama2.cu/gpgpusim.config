# Modeling a A100-like GPU in general
# Yang Yang (xqg5sq@virginia.edu)
#---------------------------------------------------------------------------------------------------------------------------------
-gpgpu_ptx_instruction_classification 0
-gpgpu_ptx_sim_mode 0
-gpgpu_ptx_force_max_capability 86                      # Ampere
-gpgpu_stack_size_limit 1024    
-gpgpu_heap_size_limit 8388608
-gpgpu_runtime_sync_depth_limit 2
-gpgpu_runtime_pending_launch_count_limit 2048
-gpgpu_kernel_launch_latency 5000                       # Kernel launch latency in cycles
-gpgpu_TB_launch_latency 0
-gpgpu_compute_capability_major 8
-gpgpu_compute_capability_minor 6
-gpgpu_ptx_convert_to_ptxplus 0
-gpgpu_ptx_save_converted_ptxplus 0
-gpgpu_shader_registers 65536                           # Number of registers per shader core. Limits number of concurrent CTAs. 
-gpgpu_registers_per_block 65536                        # Maximum number of registers per CTA.
-gpgpu_occupancy_sm_number 86
#---------------------------------------------------------------------------------------------------------------------------------
-gpgpu_shader_core_pipeline 2048:32 
-gpgpu_shader_cta 32                                    # Maximum number of concurrent CTAs in shader
-gpgpu_simd_model 1
-gpgpu_pipeline_widths 4,4,4,4,4,4,4,4,4,4,8,4,4
-gpgpu_num_sp_units 4
-gpgpu_num_sfu_units 4
-gpgpu_num_dp_units 4
-gpgpu_num_int_units 4
-gpgpu_tensor_core_avail 1
-gpgpu_num_tensor_core_units 4
#---------------------------------------------------------------------------------------------------------------------------------
-ptx_opcode_latency_int 4,13,4,5,145,21
-ptx_opcode_initiation_int 2,2,2,2,8,4
-ptx_opcode_latency_fp 4,13,4,5,39
-ptx_opcode_initiation_fp 2,2,2,2,4
-ptx_opcode_latency_dp 8,19,8,8,330
-ptx_opcode_initiation_dp 4,4,4,4,130
-ptx_opcode_latency_sfu 100
-ptx_opcode_initiation_sfu 8
-ptx_opcode_latency_tesnor 64
-ptx_opcode_initiation_tensor 64
#---------------------------------------------------------------------------------------------------------------------------------
-gpgpu_sub_core_model 1
-gpgpu_enable_specialized_operand_collector 0
-gpgpu_operand_collector_num_units_gen 8
-gpgpu_operand_collector_num_in_ports_gen 8
-gpgpu_operand_collector_num_out_ports_gen 8
-gpgpu_num_reg_banks 16                                 # Number of register banks
-gpgpu_reg_file_port_throughput 2
-gpgpu_num_sched_per_core 4
-gpgpu_scheduler gto
-gpgpu_max_insn_issue_per_warp 1
-gpgpu_dual_issue_diff_exec_units 1
#---------------------------------------------------------------------------------------------------------------------------------
## L1/shared memory configuration
# <nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>:<set_index_fn>,<mshr>:<N>:<merge>,<mq>:**<fifo_entry>
-gpgpu_adaptive_cache_config 1
-gpgpu_shmem_option 0,8,16,32,64,96
-gpgpu_unified_l1d_size 192                             # Size of unified data cache(L1D + shared memory) in KB
-gpgpu_l1_banks 4
-gpgpu_cache:dl1 S:4:128:128,L:T:m:L:L,A:384:48,16:0,32
-gpgpu_l1_cache_write_ratio 25
-gpgpu_l1_latency 15
-gpgpu_gmem_skip_L1D 0
-gpgpu_flush_l1_cache 1
-gpgpu_n_cluster_ejection_buffer_size 32
-gpgpu_shmem_size 131072
-gpgpu_shmem_sizeDefault 131072
-gpgpu_shmem_per_block 65536
-gpgpu_smem_latency 20
-gpgpu_shmem_num_banks 32
-gpgpu_shmem_limited_broadcast 0
-gpgpu_shmem_warp_parts 1
-gpgpu_coalesce_arch 86
#---------------------------------------------------------------------------------------------------------------------------------
# 32 sets, each 128 bytes 24-way for each memory sub partition (96 KB per memory sub partition). 
# 32 * 128 * 24 / 1024 = 96 KB, then 192 KB per memory partition (MC)
# This gives us 12MB L2 cache [32MC in total]
-gpgpu_cache:dl2 S:64:128:24,L:B:m:L:P,A:192:4,32:0,32

# a100 setting
# 256 * 128 * 16 = 512 KB
# 512 KB per memory sub partition (1024 KB per memory partition)
# 1MB per MC
# 40 MC -> 40 MB L2-cache
# needs test, whether we use [80sm-40mc] or [80sm-32mc] or [21sm-8mc] (shrink 1/5)
# for now 21sm - 8mc
#-gpgpu_cache:dl2 S:256:128:16,L:B:m:L:P,A:192:4,32:0,32
-gpgpu_cache:dl2_texture_only 0
-gpgpu_dram_partition_queues 64:64:64:64
-gpgpu_perf_sim_memcpy 1
-gpgpu_memory_partition_indexing 2
#---------------------------------------------------------------------------------------------------------------------------------
-gpgpu_cache:il1 N:64:128:16,L:R:f:N:L,S:2:48,4                 # 128 KB Inst.
-gpgpu_inst_fetch_throughput 4
-gpgpu_tex_cache:l1 N:4:128:256,L:R:m:N:L,T:512:8,128:2         # 128 KB Tex [deprected]
-gpgpu_const_cache:l1 N:128:64:8,L:R:f:N:L,S:2:64,4             # 64 KB Const
-gpgpu_perfect_inst_const_cache 1
#---------------------------------------------------------------------------------------------------------------------------------
-network_mode 2                                     # use built-in local xbar
-icnt_in_buffer_limit 512
-icnt_out_buffer_limit 512
-icnt_subnets 2
-icnt_flit_size 40
-icnt_arbiter_algo 1
#---------------------------------------------------------------------------------------------------------------------------------
-gpgpu_memlatency_stat 14 
-gpgpu_runtime_stat 500
-enable_ptx_file_line_stats 1
-visualizer_enabled 0
#---------------------------------------------------------------------------------------------------------------------------------
#items for functional and timing simulation of UVM ###
-gddr_size 1GB                                          # gddr size should be less than or equal to 1GB, in the unit of MB/GB
-oversub    0                                           # 0 for no oversub, hard coded inside simulator for application gddr_size, 1 for 110, 2 for 125, 3 for 150
-crypto_perc  0                                         # percentage of protection
-page_size 4KB                                          # size of gddr page, only 4KB and 2MB available
-tlb_size 4096                                          # number of tlb entries per SM
-page_table_walk_latency 100                            # average page table walk latency (in core cycle), 4K page 100, 2M page 66

# encompass the overhead of stalling threads, deciding memory address, cpu page table walk, maintaining page flags, transfer chunks and orders
# 1/f * pf_cycles = 20*10^-6 (s)
# pf_cycles = 20 * f = 20 * 901 = 18020                 # latency in core cycle to handle page fault (20us)
-page_fault_latency 18020

-invalidate_clean 0                                     # invalidate clean pages directly instead of writing back
-reserve_accessed_page_percent 0                        # reserve percentage (e.g. 10 or 20) of accesses pages from eviction
-percentage_of_free_page_buffer 0                       # percentage of free page buffer to trigger the page eviction (e.g. 5 or 10)

-pcie_bandwidth 32.0GB/s                                # per direction, full PCIe 4.0x16, 12.8GB/s for 1/5 PCIe
#-interconnet_power 8.0                                  # 8 pJ/bit for NVLink and PCIe
-sim_prof_enable 1                                      # enable/disable GMMU statistics profiling for UVM
-enable_accurate_simulation 0                           # accurate simulation for stalling warps and serializing accesses for page fault 
-gpgpu_deadlock_detect 0                                # disable deadlock check for UVM

-migrate_threshold 8                                    # Access counter threshold for migrating the page from cpu to gpu
-multiply_dma_penalty 2                                 # Oversubscription Multiplicative Penalty Factor for Adaptive DMA, pinned related [IPDPS'20]
-enable_smart_runtime 0                                 # enabling access pattern detection, policy engine, and adaptive memory management [DATE'21]
#---------------------------------------------------------------------------------------------------------------------------------
# page eviction policy
# 0 - lru 2MB (default)
# 1 - lru tree-based neighborhood
# 2 - lru sequential locality 64K
# 3 - random 4KB
# 4 - LFU 2MB
# 5 - lru 4KB
-eviction_policy 1

# prefetch only works when the application use prefetch apis
# hardware prefetcher
# 0 - disabled
# 1 - tree-based neighborhood (default)
# 2 - sequential locality 64K
# 3 - random 4 K
-hardware_prefetch 0

# hardware prefetcher under over-subscription
# 0 - disable upon eviction (default)
# 1 - tree-based neighborhood
# 2 - sequential locality 64K
# 3 - random 4 K
-hwprefetch_oversub 0

# Enable direct CPU-memory access from GPU
# 0 - disable
# 1 - adaptive
# 2 - always
# 3 - after oversubscription
# this is the pinned memory access with 200 cycles penalty
-enable_dma 0
#---------------------------------------------------------------------------------------------------------------------------------
#-dram_cache_line_size 256 
-power_simulation_enabled 0
#-accelwattch_xml_file accelwattch_ptx_sim.xml
#-power_trace_enabled 1
#-power_per_cycle_dump 0


# select lower bits for bnkgrp to increase bnkgrp parallelism
-dram_bnk_indexing_policy 0
-dram_bnkgrp_indexing_policy 1
#---------------------------------------------------------------------------------------------------------------------------------
# DRAM related configurations notice
#
# something needs to be noticed.
# the gpgpu_n_mem is the number of memory controller
# that support gpgpu_dram_buswidth bus
# which means for HBM, it controller sub-chip in a layer
#
# e.g. A100-40G-5120bit
# 5120/512 = 10 (real MC)
# every real MC controls 4 semi-layers (128 * 4 = 512)
# therefore in simulation the number of MC is 4*10 = 40
# if shrink by 1/5
# then 8 MC and 8 chips (128bit/chip)
# contributing BW = 2*1000MHz*(1024)/8 = 2000 * 128 = 256 GB/s
# that's the same if expanded to other settings 
#
# e.g. in Titan X-12G-384bit
# 384/32=12
# and the gpgpu_dram_buswidth is just 32bit
# therefore, gpgpu_n_mem = 12
#
# e.g. in QV100-32G-4096bit
# 4096/512 = 8 (real MC)
# so MC with 128bit gpgpu_dram_buswidth would be 32
# therefore, gpgpu_n_mem = 32
#
# also, as we use ramulator for dram simulation
# only clock is needed
# timing parameters are in ramulator source code.
#
# for workloads memory footprint, it would be about 19~135 MB
#---------------------------------------------------------------------------------------------------------------------------------
# high level architecture configuration
# 21 - 8 models 1/5 A100
# for a larger a100-like GPU
# 80 - 32 could be a choice
# for RTX 2060
# 30 - 12
-gpgpu_n_clusters 80
-gpgpu_n_cores_per_cluster 1
-gpgpu_n_mem 32
-gpgpu_n_sub_partition_per_mchannel 2 

-gpgpu_l2_rop_latency 120
-dram_latency 100                                       # time in the dram_latency_queue, before sending to dram
-gpgpu_dram_scheduler 1
-gpgpu_frfcfs_dram_sched_queue_size 128
-gpgpu_dram_return_queue_size 192
#---------------------------------------------------------------------------------------------------------------------------------
# HBM/PCM (3D-stack)
#
# 4 layers/stack
# 2 channels/layer
# 8 channels/stack
# 128 bits/channel [this is the number of mc in simulation]
# 1024 bits/ real memory controller
# Number of DRAM chip per memory controller (aka DRAM channel)
-gpgpu_clock_domains 901:901:901:1000                   #<Core Clock>:<Interconnect Clock>:<L2 Clock>:<DRAM Clock>, HBM/PCM: 850,1000
-gpgpu_n_mem_per_ctrlr 1
-gpgpu_dram_buswidth 16                                 # 16 B =  128 bit
-gpgpu_dram_burst_length 2                              # 2 for HBM
-dram_data_command_freq_ratio 2                         # HBM is DDR 
-gpgpu_mem_address_mask 1
-gpgpu_mem_addr_mapping dramid@8;00000000.00000000.00000000.00000000.0000RRRR.RRRRRRRR.RBBBCCCB.CCCSSSSS

# not needed if use ramulator
#-gpgpu_mem_addr_mapping dramid@8;RRRRRRRR.RRRRRRRR.RRRRRRRR.RRRRRRRR.RRRRRRRR.RRRRRRRR.BBBCCCHB.CCCSSSSS

# not needed if use ramulator
# Timing for 850 MHZ
#-gpgpu_dram_timing_opt "nbk=16:CCD=1:RRD=3:RCD=12:RAS=28:RP=12:RC=40:CL=12:WL=2:CDLR=3:WR=10:nbkgrp=4:CCDL=2:RTPL=3"
# Timing for 1 GHZ 
-gpgpu_dram_timing_opt "nbk=16:CCD=1:RRD=4:RCD=14:RAS=33:RP=14:RC=47:CL=14:WL=2:CDLR=3:WR=12:nbkgrp=4:CCDL=2:RTPL=4"
# HBM has dual bus interface, in which it can issue two col and row commands at a time
-dram_dual_bus_interface 1
#---------------------------------------------------------------------------------------------------------------------------------
# GDDR5
#-gpgpu_clock_domains 901:901:901:1500                   #<Core Clock>:<Interconnect Clock>:<L2 Clock>:<DRAM Clock>, GDDR5-6000:1500
#-gpgpu_n_mem_per_ctrlr 1
#-gpgpu_dram_buswidth 4                                 # 32 bit
#-gpgpu_dram_burst_length 8                             # 8 for GDDR5
#-dram_data_command_freq_ratio 4                        # QDR
#-gpgpu_mem_address_mask 1
#-gpgpu_mem_addr_mapping dramid@8;RRRRRRRR.RRRRRRRR.RRRRRRRR.RRRRRRRR.RRRRRRRR.RRRRRRRR.RBBBCCCC.BCCSSSSS
#-gpgpu_dram_timing_opt "nbk=16:CCD=2:RRD=8:RCD=16:RAS=37:RP=16:RC=52:CL=16:WL=6:CDLR=7:WR=16:nbkgrp=4:CCDL=4:RTPL=3"
#-dram_dual_bus_interface 0
#---------------------------------------------------------------------------------------------------------------------------------
# GDDR6
#-gpgpu_clock_domains 901:901:901:3500                   #<Core Clock>:<Interconnect Clock>:<L2 Clock>:<DRAM Clock>, 1365 for Core
#-gpgpu_n_mem_per_ctrlr 1
#-gpgpu_dram_buswidth 2                                 # 16 bit, 12 MC/16bit
#-gpgpu_dram_burst_length 16                            # 16 for GDDR6
#-dram_data_command_freq_ratio 4                        # QDR
#-gpgpu_mem_address_mask 1
#-gpgpu_mem_addr_mapping dramid@8;RRRRRRRR.RRRRRRRR.RRRRRRRR.RRRRRRRR.RRRRRRRR.RRRRRRRR.RBBBCCCC.BCCSSSSS
#-gpgpu_dram_timing_opt "nbk=16:CCD=4:RRD=10:RCD=20:RAS=50:RP=20:RC=62:CL=20:WL=8:CDLR=9:WR=20:nbkgrp=4:CCDL=4:RTPL=4"
#-dram_dual_bus_interface 0
#---------------------------------------------------------------------------------------------------------------------------------
-use_xom 0
-use_ramulator 0
-ramulator_mem_type HBM # HBM, PCM, GDDR5